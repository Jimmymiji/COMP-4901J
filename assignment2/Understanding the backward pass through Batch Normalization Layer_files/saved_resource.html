<!DOCTYPE html>
<!-- saved from url=(0414)https://disqus.com/embed/comments/?base=default&f=kratzertblog&t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&s_o=default#version=87588b32779e4f2b9c990292ddde7ffd -->
<html lang="en" dir="ltr" class="js no-touch localstorage sessionstorage contenteditable use-opacity-transitions"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Disqus Comments</title>

    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <style>
        .alert--warning {
            border-radius: 3px;
            padding: 10px 15px;
            margin-bottom: 10px;
            background-color: #FFE070;
            color: #A47703;
        }

        .alert--warning a,
        .alert--warning a:hover,
        .alert--warning strong {
            color: #A47703;
            font-weight: bold;
        }

        .alert--error p,
        .alert--warning p {
            margin-top: 5px;
            margin-bottom: 5px;
        }
        
        </style>
    
    <style>
        
        html {
            overflow: hidden;
        }
        

        #error {
            display: none;
        }

        .clearfix:after {
            content: "";
            display: block;
            height: 0;
            clear: both;
            visibility: hidden;
        }

        
    </style>

<script src="./cb=gapi.loaded_0" async=""></script><script crossorigin="anonymous" id="bootstrap-script" data-app="lounge" src="./lounge.load.87588b32779e4f2b9c990292ddde7ffd.js.下载"></script><meta http-equiv="Content-Security-Policy" content="script-src https:;"><link rel="stylesheet" href="./lounge.d797d52db05c56e7ec33542889f90bca.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="lounge/main" src="./lounge.bundle.37ca27bb3049421f2832eed5d09cfc6b.js.下载"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="remote/config" src="./config.js.下载"></script><style id="css_1538726212717"></style><!--<base target="_parent">--><base href="." target="_parent"><link rel="stylesheet" href="./discovery.539723090a04fba1b162858415868357.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="discovery/main" src="./discovery.bundle.daedd146972fc7d8dffd9be34c404865.js.下载"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="fb" src="./sdk.js.下载"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="gapi" src="./api.js.下载" gapi_processed="true"></script><style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}.fb_link img{border:none}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_reset .fb_dialog_legacy{overflow:visible}.fb_dialog_advanced{padding:10px;border-radius:8px}.fb_dialog_content{background:#fff;color:#333}.fb_dialog_close_icon{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{top:5px;left:5px;right:auto}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_loader{background-color:#f5f6f7;border:1px solid #606060;font-size:24px;padding:20px}.fb_dialog_top_left,.fb_dialog_top_right,.fb_dialog_bottom_left,.fb_dialog_bottom_right{height:10px;width:10px;overflow:hidden;position:absolute}.fb_dialog_top_left{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 0;left:-10px;top:-10px}.fb_dialog_top_right{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -10px;right:-10px;top:-10px}.fb_dialog_bottom_left{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -20px;bottom:-10px;left:-10px}.fb_dialog_bottom_right{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ye/r/8YeTNIlTZjm.png) no-repeat 0 -30px;right:-10px;bottom:-10px}.fb_dialog_vert_left,.fb_dialog_vert_right,.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{position:absolute;background:#525252;filter:alpha(opacity=70);opacity:.7}.fb_dialog_vert_left,.fb_dialog_vert_right{width:10px;height:100%}.fb_dialog_vert_left{margin-left:-10px}.fb_dialog_vert_right{right:0;margin-right:-10px}.fb_dialog_horiz_top,.fb_dialog_horiz_bottom{width:100%;height:10px}.fb_dialog_horiz_top{margin-top:-10px}.fb_dialog_horiz_bottom{bottom:0;margin-bottom:-10px}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{-webkit-transform:none;height:100%;margin:0;overflow:visible;position:absolute;top:-10000px;left:0;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{width:auto;height:auto;min-height:initial;min-width:initial;background:none}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{color:#fff;display:block;padding-top:20px;clear:both;font-size:18px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .45);position:absolute;bottom:0;left:0;right:0;top:0;width:100%;min-height:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:-webkit-sticky;top:0}.fb_dialog_content .dialog_header{-webkit-box-shadow:white 0 1px 1px -1px inset;background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#738ABA), to(#2C4987));border-bottom:1px solid;border-color:#1d4088;color:#fff;font:14px Helvetica, sans-serif;font-weight:bold;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{-webkit-font-smoothing:subpixel-antialiased;height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:-webkit-gradient(linear, 0% 0%, 0% 100%, from(#4966A6), color-stop(.5, #355492), to(#2A4887));border:1px solid #29487d;-webkit-background-clip:padding-box;-webkit-border-radius:3px;-webkit-box-shadow:rgba(0, 0, 0, .117188) 0 1px 1px inset, rgba(255, 255, 255, .167969) 0 1px 0;display:inline-block;margin-top:3px;max-width:85px;line-height:18px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{border:none;background:none;color:#fff;font:12px Helvetica, sans-serif;font-weight:bold;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #555;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #555;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://static.xx.fbcdn.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-repeat:no-repeat;background-position:50% 50%;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_hide_iframes iframe{position:relative;left:-10000px}.fb_iframe_widget_loader{position:relative;display:inline-block}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}.fb_iframe_widget_loader iframe{min-height:32px;z-index:2;zoom:1}.fb_iframe_widget_loader .FB_Loader{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat;height:32px;width:32px;margin-left:-16px;position:absolute;left:50%;z-index:4}
.fb_customer_chat_bounce_in_v2{animation-duration:300ms;animation-name:fb_bounce_in_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_v2{animation-duration:300ms;animation-name:fb_bounce_out_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_in_v2_mobile_chat_started{animation-duration:300ms;animation-name:fb_bounce_in_v2_mobile_chat_started;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_v2_mobile_chat_started{animation-duration:300ms;animation-name:fb_bounce_out_v2_mobile_chat_started;transition-timing-function:ease-in}.fb_customer_chat_bubble_pop_in{animation-duration:250ms;animation-name:fb_customer_chat_bubble_bounce_in_animation}.fb_customer_chat_bubble_animated_no_badge{box-shadow:0 3px 12px rgba(0, 0, 0, .15);transition:box-shadow 150ms linear}.fb_customer_chat_bubble_animated_no_badge:hover{box-shadow:0 5px 24px rgba(0, 0, 0, .3)}.fb_customer_chat_bubble_animated_with_badge{box-shadow:-5px 4px 14px rgba(0, 0, 0, .15);transition:box-shadow 150ms linear}.fb_customer_chat_bubble_animated_with_badge:hover{box-shadow:-5px 8px 24px rgba(0, 0, 0, .2)}.fb_invisible_flow{display:inherit;height:0;overflow-x:hidden;width:0}.fb_mobile_overlay_active{background-color:#fff;height:100%;overflow:hidden;position:fixed;visibility:hidden;width:100%}@keyframes fb_bounce_in_v2{0%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}50%{transform:scale(1.03, 1.03);transform-origin:bottom right}100%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}}@keyframes fb_bounce_in_v2_mobile_chat_started{0%{opacity:0;top:20px}100%{opacity:1;top:0}}@keyframes fb_bounce_out_v2{0%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}100%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}}@keyframes fb_bounce_out_v2_mobile_chat_started{0%{opacity:1;top:0}100%{opacity:0;top:20px}}@keyframes fb_customer_chat_bubble_bounce_in_animation{0%{bottom:6pt;opacity:0;transform:scale(0, 0);transform-origin:center}70%{bottom:18pt;opacity:1;transform:scale(1.2, 1.2)}100%{transform:scale(1, 1)}}</style></head>
<body class="serif">
    

    
    <div id="error" class="alert--error">
        <p>We were unable to load Disqus. If you are a moderator please see our <a href="https://docs.disqus.com/help/83/"> troubleshooting guide</a>. </p>
    </div>

    
    <script type="text/json" id="disqus-forumData">{"session":{"canModerate":false,"audienceSyncVerified":false,"mustVerify":false,"canReply":true,"mustVerifyEmail":false},"forum":{"aetBannerConfirmation":null,"founder":"196251915","twitterName":null,"commentsLinkOne":"1 Comment","guidelines":null,"favicon":{"permalink":"https://disqus.com/api/forums/favicons/kratzertblog.jpg","cache":"//a.disquscdn.com/1537216441/images/favicon-default.png"},"commentsLinkZero":"0 Comments","disableDisqusBranding":false,"daysAlive":0,"createdAt":"2016-02-12T06:01:19.484948","category":"Tech","aetBannerEnabled":false,"aetBannerTitle":null,"raw_guidelines":null,"colorScheme":"auto","id":"kratzertblog","installCompleted":true,"moderatorBadgeText":"","commentPolicyText":null,"aetEnabled":false,"channel":null,"sort":4,"description":null,"newPolicy":true,"raw_description":null,"language":"en","adsReviewStatus":0,"pk":"4023296","forumCategory":{"date_added":"2016-01-28T01:54:31","id":8,"name":"Tech"},"permissions":{},"commentPolicyLink":null,"aetBannerDescription":null,"name":"kratzertblog","commentsLinkMultiple":"{num} Comments","settings":{"adsDRNativeEnabled":false,"disable3rdPartyTrackers":false,"adsVideoEnabled":false,"allowAnonPost":false,"audienceSyncEnabled":false,"unapproveLinks":false,"linkAffiliationEnabled":true,"adsProductLinksThumbnailsEnabled":false,"adsProductStoriesEnabled":false,"organicDiscoveryEnabled":true,"adsProductDisplayEnabled":false,"adsProductLinksEnabled":false,"hasCustomAvatar":false,"adsEnabled":false,"threadReactionsEnabled":false,"adsPositionTopEnabled":false,"allowMedia":true,"adultContent":false,"allowAnonVotes":false,"adsProductVideoEnabled":false,"mustVerify":true,"mustVerifyEmail":true,"ssoRequired":false,"mediaembedEnabled":true,"adsPositionBottomEnabled":false,"discoveryLocked":false,"validateAllPosts":false,"adsSettingsLocked":false,"isVIP":false,"adsPositionInthreadEnabled":false},"organizationId":2907269,"typeface":"auto","url":"","daysThreadAlive":0,"avatar":{"small":{"permalink":"https://disqus.com/api/forums/avatars/kratzertblog.jpg?size=32","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"large":{"permalink":"https://disqus.com/api/forums/avatars/kratzertblog.jpg?size=92","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}},"signedUrl":""}}</script>

    <script type="text/json" id="disqus-threadData">{"cursor":{"hasPrev":false,"prev":null,"total":114,"hasNext":true,"next":"1:0:0"},"code":0,"response":{"lastModified":1538638246,"posts":[{"editableUntil":"2018-01-01T15:28:27","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>hi\uff0c\u003cbr>I believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.\u003c/p>\u003cp>\"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions).\"\u003c/p>\u003cp>I think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.\u003c/p>","id":"3677924058","createdAt":"2017-12-25T15:28:27","author":{"username":"disqus_lDJKDl7VxM","about":"","name":"JohnMclain","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-25T15:24:57","profileUrl":"https://disqus.com/by/disqus_lDJKDl7VxM/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275048753","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"hi\uff0c\nI believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.\n\n\"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions).\"\n\nI think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":2,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-01-22T08:40:37","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).\u003c/p>","id":"3708575158","createdAt":"2018-01-15T08:40:37","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3677924058,"isApproved":true,"isFlagged":false,"raw_message":"Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-07T09:04:18","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew).\u003c/p>\u003cp>I think the reason for adding up rows is the multivariate-calculus rule which says \"gradients flow into different branches of the graph should be summed\". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable.\u003c/p>\u003cp>As a concrete example, in step 9, a term of dL/d\\beta should be:\u003cbr> \u003ca href=\"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif\">https://uploads.disquscdn.c...\u003c/a> \u003cbr>where z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.\u003c/p>\u003cp>However, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?\u003c/p>","id":"3879045238","createdAt":"2018-04-30T09:04:18","author":{"username":"harveyqiu","about":"","name":"Harvey Qiu","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-30T08:56:20","profileUrl":"https://disqus.com/by/harveyqiu/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"286645466","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar32.jpg?1525078584"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar92.jpg?1525078584","large":{"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar92.jpg?1525078584"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thumbnailUrl":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","htmlHeight":null,"id":40228045,"thumbnailWidth":171,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif"},"urlRedirect":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","description":"","post":"3879045238","thumbnailURL":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thumbnailHeight":49}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3708575158,"isApproved":true,"isFlagged":false,"raw_message":"Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew). \n\nI think the reason for adding up rows is the multivariate-calculus rule which says \"gradients flow into different branches of the graph should be summed\". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable. \n\nAs a concrete example, in step 9, a term of dL/d\\beta should be:\n https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif \nwhere z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.\n\nHowever, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-05-14T06:42:04","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Harvey,\u003cbr>sorry for responding so late. I think the reason you give is absolutely correct but I'm not sure if I get your last question. You mean why I started to disassemble everything in small steps? I saw it in one of the lectures Andrej Kaparthy gave during the CS231n class I linked at the top.  He did the same to explain the backward pass an gradient calculation and I think it is a nice way to make things easier ;)\u003c/p>","id":"3889201765","createdAt":"2018-05-07T06:42:04","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3879045238,"isApproved":true,"isFlagged":false,"raw_message":"Hey Harvey,\nsorry for responding so late. I think the reason you give is absolutely correct but I'm not sure if I get your last question. You mean why I started to disassemble everything in small steps? I saw it in one of the lectures Andrej Kaparthy gave during the CS231n class I linked at the top.  He did the same to explain the backward pass an gradient calculation and I think it is a nice way to make things easier ;)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":3,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-06T21:19:10","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>Thanks ! the flowchart help me so much to implement in R ;-)\u003cbr>\u003ca href=\"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>","id":"3638799277","createdAt":"2017-11-29T21:19:10","author":{"username":"disqus_cxY7uKUvpl","about":"","name":"aboul","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-11-29T21:09:14","profileUrl":"https://disqus.com/by/disqus_cxY7uKUvpl/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"272639134","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar32.jpg?1511990351"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar92.jpg?1511990351","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar92.jpg?1511990351"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thumbnailUrl":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","htmlHeight":null,"id":34057645,"thumbnailWidth":3192,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg"},"urlRedirect":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","description":"","post":"3638799277","thumbnailURL":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thumbnailHeight":2376}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks ! the flowchart help me so much to implement in R ;-)\nhttps://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":2,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-07T08:58:30","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Looks like my sketches, when I first sat down to derive the graph for my self. Good work!\u003c/p>","id":"3639479740","createdAt":"2017-11-30T08:58:30","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3638799277,"isApproved":true,"isFlagged":false,"raw_message":"Looks like my sketches, when I first sat down to derive the graph for my self. Good work!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-02T07:18:47","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hi Frederik,\u003c/p>\u003cp>Thanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout.\u003c/p>\u003cp>From what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!\u003c/p>\u003cp>Patrick\u003c/p>\u003cp>==========\u003cbr>Edit: Never mind. I found the explanation on the link you gave at the end of the post \u003ca href=\"http://disq.us/url?url=http%3A%2F%2Fcthorey.github.io%2Fbackpropagation%2F%3Ar33DVWl8q8rDRF9zRhyrL5527fg&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://cthorey.github.io/backpropagation/\">http://cthorey.github.io/ba...\u003c/a> in the section \"We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...\". This is related to the notion of \"total derivative\" \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTotal_derivative%3AdO1EtjQujaspBuHilHbsqfsJopg&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://en.wikipedia.org/wiki/Total_derivative\">https://en.wikipedia.org/wi...\u003c/a>.\u003c/p>\u003cp>In addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,\u003c/p>","id":"3678638435","createdAt":"2017-12-26T07:18:47","author":{"username":"patricklangechuanliu","about":"","name":"Patrick Langechuan LIU","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-26T07:12:30","profileUrl":"https://disqus.com/by/patricklangechuanliu/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275100268","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3639479740,"isApproved":true,"isFlagged":false,"raw_message":"Hi Frederik,\n\nThanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout. \n\nFrom what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!\n\nPatrick\n\n==========\nEdit: Never mind. I found the explanation on the link you gave at the end of the post http://cthorey.github.io/backpropagation/ in the section \"We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...\". This is related to the notion of \"total derivative\" https://en.wikipedia.org/wiki/Total_derivative.\n\nIn addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2018-03-02T18:41:11","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks fkratzert for your effort !!\u003cbr>I verified your code passes Andrew NG's Gradient Checking.\u003cbr>Cheers!!\u003c/p>","id":"3772850279","createdAt":"2018-02-23T18:41:11","author":{"username":"disqus_GJH1JRIogE","about":"","name":"Jorge","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-02-23T18:24:37","profileUrl":"https://disqus.com/by/disqus_GJH1JRIogE/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"280979018","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks fkratzert for your effort !!\nI verified your code passes Andrew NG's Gradient Checking.\nCheers!!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-02T19:00:47","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks man, good to hear ;)\u003c/p>","id":"3772882856","createdAt":"2018-02-23T19:00:47","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3772850279,"isApproved":true,"isFlagged":false,"raw_message":"Thanks man, good to hear ;)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T16:34:25","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thank you! You have a knack of simplifying complexity :-)\u003c/p>","id":"3579296064","createdAt":"2017-10-22T16:34:25","author":{"username":"anandsaha","about":"","name":"Anand Saha","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2011-02-08T03:40:38","profileUrl":"https://disqus.com/by/anandsaha/","url":"http://teleported.in","location":"Pune, India","isPrivate":true,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fteleported.in&key=h4uu9ADYgxyWuK2xjbQSQQ","isPrimary":true,"isAnonymous":false,"id":"7128646","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar32.jpg?1493544799"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar92.jpg?1493544799","large":{"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar92.jpg?1493544799"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thank you! You have a knack of simplifying complexity :-)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T07:16:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.\u003c/p>","id":"3580214271","createdAt":"2017-10-23T07:16:13","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3579296064,"isApproved":true,"isFlagged":false,"raw_message":"Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-18T07:15:25","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.\u003c/p>","id":"3561820127","createdAt":"2017-10-11T07:15:25","author":{"username":"majicji","about":"","name":"Majic Ji","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-10-11T07:15:12","profileUrl":"https://disqus.com/by/majicji/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"267800662","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T08:31:07","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Glad I could help you.\u003c/p>","id":"3578840984","createdAt":"2017-10-22T08:31:07","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3561820127,"isApproved":true,"isFlagged":false,"raw_message":"Glad I could help you.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T02:49:36","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.\u003cbr>Thanks!\u003c/p>","id":"3268047638","createdAt":"2017-04-22T02:49:36","author":{"username":"jschaeff","about":"","name":"jschaeff","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-04-22T02:17:36","profileUrl":"https://disqus.com/by/jschaeff/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"249599616","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.\nThanks!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T14:23:28","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? \u003cbr>And regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum.\u003c/p>\u003cp>And for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?\u003c/p>","id":"3268555117","createdAt":"2017-04-22T14:23:28","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268047638,"isApproved":true,"isFlagged":false,"raw_message":"Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? \nAnd regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum. \n\nAnd for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T15:22:44","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!\u003c/p>\u003cp>For the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?\u003c/p>\u003cp>Thanks again\u003c/p>","id":"3268622523","createdAt":"2017-04-22T15:22:44","author":{"username":"jschaeff","about":"","name":"jschaeff","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-04-22T02:17:36","profileUrl":"https://disqus.com/by/jschaeff/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"249599616","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268555117,"isApproved":true,"isFlagged":false,"raw_message":"Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!\n\nFor the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?\n\nThanks again","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T15:28:16","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>So normally the running mean + std from the training data are parameters that should be included in the pretrained weights. These are network parameters, same as gamma and beta. So you don't have to compute them or pass them on each forward pass during inference to the network. As to my knowledge this is done automatically for all of the DL libraries. For TensorFlow e.g. you just have to pass a flag to the BatchNorm-Layer if you are currently training or testing. If you are testing, then the stored moving average will be taken, if not, the moving average will be updated. But again, these parameters are network parameters and should be provided for a pretrained network.\u003c/p>","id":"3268629001","createdAt":"2017-04-22T15:28:16","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268622523,"isApproved":true,"isFlagged":false,"raw_message":"So normally the running mean + std from the training data are parameters that should be included in the pretrained weights. These are network parameters, same as gamma and beta. So you don't have to compute them or pass them on each forward pass during inference to the network. As to my knowledge this is done automatically for all of the DL libraries. For TensorFlow e.g. you just have to pass a flag to the BatchNorm-Layer if you are currently training or testing. If you are testing, then the stored moving average will be taken, if not, the moving average will be updated. But again, these parameters are network parameters and should be provided for a pretrained network.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":3,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T16:26:47","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Got it.  Thanks again for your clear explanations.\u003c/p>","id":"3268702865","createdAt":"2017-04-22T16:26:47","author":{"username":"jschaeff","about":"","name":"jschaeff","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-04-22T02:17:36","profileUrl":"https://disqus.com/by/jschaeff/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"249599616","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268629001,"isApproved":true,"isFlagged":false,"raw_message":"Got it.  Thanks again for your clear explanations.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":4,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T16:27:46","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>You're welcome! Always happy if I can help!\u003c/p>","id":"3268704115","createdAt":"2017-04-22T16:27:46","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268702865,"isApproved":true,"isFlagged":false,"raw_message":"You're welcome! Always happy if I can help!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":5,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-18T06:09:07","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Nice writeup. :)\u003c/p>","id":"3895466348","createdAt":"2018-05-11T06:09:07","author":{"username":"disqus_dVXj63cxog","about":"","name":"Michael Green","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-01-10T20:19:08","profileUrl":"https://disqus.com/by/disqus_dVXj63cxog/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"238245013","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar32.jpg?1514806084"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar92.jpg?1514806084","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar92.jpg?1514806084"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Nice writeup. :)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-18T07:52:44","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks :)\u003c/p>","id":"3895523131","createdAt":"2018-05-11T07:52:44","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3895466348,"isApproved":true,"isFlagged":false,"raw_message":"Thanks :)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-03T18:53:42","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks for the post I loved it :D\u003c/p>","id":"3874235827","createdAt":"2018-04-26T18:53:42","author":{"username":"abhisheknadgeri","about":"","name":"Abhishek Nadgeri","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-26T18:53:35","profileUrl":"https://disqus.com/by/abhisheknadgeri/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"286384918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the post I loved it :D","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-14T06:37:23","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>you are welcome. great that you liked it!\u003c/p>","id":"3889199453","createdAt":"2018-05-07T06:37:23","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3874235827,"isApproved":true,"isFlagged":false,"raw_message":"you are welcome. great that you liked it!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-04-13T20:42:11","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term \"derivative\", whereas \"derivation\" instead refers to the process of arriving at a result.\u003c/p>","id":"3842539330","createdAt":"2018-04-06T20:42:11","author":{"username":"ryanneph","about":"","name":"Ryan Neph","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-06T20:41:55","profileUrl":"https://disqus.com/by/ryanneph/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"284757841","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term \"derivative\", whereas \"derivation\" instead refers to the process of arriving at a result.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-14T06:36:55","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Ryan,\u003cbr>sorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.\u003c/p>","id":"3889199235","createdAt":"2018-05-07T06:36:55","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3842539330,"isApproved":true,"isFlagged":false,"raw_message":"Hey Ryan,\nsorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-26T05:01:10","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Wow this is such amazing post! Thank you so much!\u003c/p>","id":"3812959741","createdAt":"2018-03-19T05:01:10","author":{"username":"jae_duk_seo","about":"Hello! Currently a fourth year comp sci student","name":"Jae Duk Seo","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-01-12T14:21:42","profileUrl":"https://disqus.com/by/jae_duk_seo/","url":"https://jaedukseo.me/","location":"Toronto, ON, Canada","isPrivate":false,"signedUrl":"https://disq.us/?url=https%3A%2F%2Fjaedukseo.me%2F&key=keRUg22y-Ty-EkyxWD2LbQ","isPrimary":true,"isAnonymous":false,"id":"276840001","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar32.jpg?1524419412"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412","large":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Wow this is such amazing post! Thank you so much!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-03T08:09:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.\u003cbr>Loved the clarity of explanation.\u003c/p>","id":"3773765640","createdAt":"2018-02-24T08:09:13","author":{"username":"vibhujawa","about":"","name":"Vibhu Jawa","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-07-04T09:59:58","profileUrl":"https://disqus.com/by/vibhujawa/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"113115540","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.\nLoved the clarity of explanation.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-04T18:05:25","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work\u003c/p>","id":"3775664434","createdAt":"2018-02-25T18:05:25","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3773765640,"isApproved":true,"isFlagged":false,"raw_message":"Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-02-15T19:05:06","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Amazing post, thank you for posting!\u003c/p>","id":"3749093115","createdAt":"2018-02-08T19:05:06","author":{"username":"jae_duk_seo","about":"Hello! Currently a fourth year comp sci student","name":"Jae Duk Seo","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-01-12T14:21:42","profileUrl":"https://disqus.com/by/jae_duk_seo/","url":"https://jaedukseo.me/","location":"Toronto, ON, Canada","isPrivate":false,"signedUrl":"https://disq.us/?url=https%3A%2F%2Fjaedukseo.me%2F&key=keRUg22y-Ty-EkyxWD2LbQ","isPrimary":true,"isAnonymous":false,"id":"276840001","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar32.jpg?1524419412"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412","large":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Amazing post, thank you for posting!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-02-09T14:05:21","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>What a lovely post \u315c\u315c\u003c/p>","id":"3738782013","createdAt":"2018-02-02T14:05:21","author":{"username":"hyeungshikjung","about":"","name":"Hyeungshik Jung","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2013-06-07T00:26:12","profileUrl":"https://disqus.com/by/hyeungshikjung/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"55165620","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar32.jpg?1525912589"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar92.jpg?1525912589","large":{"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar92.jpg?1525912589"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"What a lovely post \u315c\u315c","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-06T02:15:06","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>I have a problem with \"give me back my raw inputs\" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.\u003c/p>","id":"3683913718","createdAt":"2017-12-30T02:15:06","author":{"username":"tapsi_789","about":"","name":"tapsi","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-30T01:54:32","profileUrl":"https://disqus.com/by/tapsi_789/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275457335","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"I have a problem with \"give me back my raw inputs\" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-01-11T09:51:19","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.\u003c/p>","id":"3691076340","createdAt":"2018-01-04T09:51:19","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3683913718,"isApproved":true,"isFlagged":false,"raw_message":"No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-12T15:10:25","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.\u003c/p>","id":"3693075455","createdAt":"2018-01-05T15:10:25","author":{"username":"tapsi_789","about":"","name":"tapsi","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-30T01:54:32","profileUrl":"https://disqus.com/by/tapsi_789/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275457335","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3691076340,"isApproved":true,"isFlagged":false,"raw_message":"Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-15T08:35:29","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Would be nice to hear of your results, once you can show off something (also negative results please ;) )\u003c/p>","id":"3697166058","createdAt":"2018-01-08T08:35:29","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3693075455,"isApproved":true,"isFlagged":false,"raw_message":"Would be nice to hear of your results, once you can show off something (also negative results please ;) )","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":3,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-29T22:20:23","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Awesome work, helped a lot, thank you buddy..\u003c/p>","id":"3675016359","createdAt":"2017-12-22T22:20:23","author":{"username":"kanhavishva","about":"","name":"kanhavishva","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-29T10:59:13","profileUrl":"https://disqus.com/by/kanhavishva/","url":"","location":"","isPrivate":true,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"159548160","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar32.jpg?1522092996"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar92.jpg?1522092996","large":{"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar92.jpg?1522092996"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Awesome work, helped a lot, thank you buddy..","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-30T19:54:14","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too\u003c/p>","id":"3676081514","createdAt":"2017-12-23T19:54:14","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3675016359,"isApproved":true,"isFlagged":false,"raw_message":"Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-01T18:11:49","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??\u003c/p>","id":"3631354318","createdAt":"2017-11-24T18:11:49","author":{"username":"rakhilimmidisetti","about":"","name":"Rakhil Immidisetti","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-11-24T18:11:33","profileUrl":"https://disqus.com/by/rakhilimmidisetti/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"272151587","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar32.jpg?1511547112"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar92.jpg?1511547112","large":{"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar92.jpg?1511547112"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-11-29T06:14:50","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Cool, thanks.\u003c/p>","id":"3627668880","createdAt":"2017-11-22T06:14:50","author":{"username":"zhuzii","about":"","name":"zhuzii","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-02-21T05:30:34","profileUrl":"https://disqus.com/by/zhuzii/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"197538510","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar32.jpg?1511331292"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar92.jpg?1511331292","large":{"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar92.jpg?1511331292"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Cool, thanks.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-23T21:19:38","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.\u003c/p>\u003cp>The description is at \u003ca href=\"http://disq.us/url?url=http%3A%2F%2FmiabellaAI.net%2F%3AmZg9hp0mP9jfL2lQ_spC2Vavfxs&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://miabellaAI.net/\">http://miabellaAI.net/\u003c/a> and the web app is here: \u003ca href=\"http://disq.us/url?url=http%3A%2F%2Fann.miabellaAI.net%2F%3AXkKD8PC7tnMuxD7JHLis1E4SN3Q&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://ann.miabellaAI.net/\">http://ann.miabellaAI.net/\u003c/a>\u003c/p>\u003cp>Here are some tips for those who took Andrew Ng\u2019s deep learning course:\u003c/p>\u003cp>1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.\u003c/p>\u003cp>2. The Python code I\u2019ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.\u003c/p>\u003cp>3. Gradient checking can be a pain to write, but it\u2019s the main way to make sure everything is hooked up correctly.\u003c/p>","id":"3570397972","createdAt":"2017-10-16T21:19:38","author":{"username":"disqus_nZ5GrhxFcX","about":"","name":"Writer","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-06-28T00:58:53","profileUrl":"https://disqus.com/by/disqus_nZ5GrhxFcX/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"112372310","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar32.jpg?1403917655"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar92.jpg?1403917655","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar92.jpg?1403917655"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.\n\nThe description is at http://miabellaAI.net/ and the web app is here: http://ann.miabellaAI.net/\n\nHere are some tips for those who took Andrew Ng\u2019s deep learning course:\n\n1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.\n\n2. The Python code I\u2019ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.\n\n3. Gradient checking can be a pain to write, but it\u2019s the main way to make sure everything is hooked up correctly.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T11:23:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fkevinzakka.github.io%2F2016%2F09%2F14%2Fbatch_normalization%2F%3AtMo4ZSbuJOQUKdX33UaO9WftIFM&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://kevinzakka.github.io/2016/09/14/batch_normalization/\">https://kevinzakka.github.i...\u003c/a> and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.\u003c/p>","id":"3531385013","createdAt":"2017-09-23T11:23:13","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: https://kevinzakka.github.io/2016/09/14/batch_normalization/ and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T15:28:56","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Samuel,\u003cbr>there are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. \u003cbr>E.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating \u003cbr>f'(x) = (f(x + delta_x) - f(x)) / delta_x\u003cbr>and then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.\u003c/p>","id":"3531630005","createdAt":"2017-09-23T15:28:56","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3531385013,"isApproved":true,"isFlagged":false,"raw_message":"Hey Samuel,\nthere are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. \nE.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating \nf'(x) = (f(x + delta_x) - f(x)) / delta_x\nand then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T17:30:57","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!\u003c/p>","id":"3531808372","createdAt":"2017-09-23T17:30:57","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3531630005,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":2,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T08:27:21","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>let me know if it worked for you!\u003c/p>","id":"3578839022","createdAt":"2017-10-22T08:27:21","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3531808372,"isApproved":true,"isFlagged":false,"raw_message":"let me know if it worked for you!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":3,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T16:58:30","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Dear Fkratzert,\u003c/p>\u003cp>Thank you very much for following up with me. Again I am a newbie I guess there is a high chance that I was wrong. I spent quite some time on this issue last month but I still couldnt figure out what went wrong.\u003c/p>\u003cp>I tried 2 approaches and I uploaded my code here:\u003cbr>\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Fchihoxtra%2FMachineLearning%2Fblob%2Fmaster%2F%28Local%29%2520Neuro%2520Network%2520PlayAround.ipynb%3AV7JY7q1_eimdpzwk0dovYf97CtQ&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://github.com/chihoxtra/MachineLearning/blob/master/(Local)%20Neuro%20Network%20PlayAround.ipynb\">https://github.com/chihoxtr...\u003c/a>\u003c/p>\u003cp>First I basically refer to your approach for both batchnorm forward and backward propagation, the particular functions are:\u003cbr>batchnorm_forward_computational_graph\u003cbr>batchnorm_backward_computational_graph\u003c/p>\u003cp>while the forward one seems to be working really well, the backward one doesnt work thru the gradient check. just in case if you wanted to try to run my stupid code, you can set the following parameter to \"True\": (it is located at the end of the code)\u003cbr>global_var['checkGradient'] = True\u003c/p>\u003cp>Note that the gradient check need a few iterations to work ok. After some iterations you will see the difference between computational by backward propagation and that of \"manually\" calculating the slope become very small, here is a screenshot:\u003cbr> \u003ca href=\"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>\u003cp>Then I referred to some other examples online and found one that works for the gradient check. I implemented it and now it passes the gradient check. The function is this time around is called:\u003cbr>batchnorm_backward_thankGod\u003cbr>(PS i thankGod for finally be able to pass the checking). As you can see the coding there is way more abstract. I spent quite some time understand the maths behind and the codes. The original source was from: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fwiseodd.github.io%2Ftechblog%2F2016%2F07%2F04%2Fbatchnorm%2F%3ARK0L7aVOAtxP6h4cXjYYKZc3tuU&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\">https://wiseodd.github.io/t...\u003c/a>\u003c/p>\u003cp>I am sure there could be something wrong here but just if you have time, do you mind helping me take a quick look? sorry for the messy codes.\u003c/p>\u003cp>Sam\u003c/p>","id":"3579327693","createdAt":"2017-10-22T16:58:30","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","thumbnailUrl":"//uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","htmlHeight":null,"id":32529205,"thumbnailWidth":768,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png"},"urlRedirect":"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","description":"","post":"3579327693","thumbnailURL":"//uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png","thumbnailHeight":323}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3578839022,"isApproved":true,"isFlagged":false,"raw_message":"Dear Fkratzert,\n\nThank you very much for following up with me. Again I am a newbie I guess there is a high chance that I was wrong. I spent quite some time on this issue last month but I still couldnt figure out what went wrong.\n\nI tried 2 approaches and I uploaded my code here:\nhttps://github.com/chihoxtra/MachineLearning/blob/master/(Local)%20Neuro%20Network%20PlayAround.ipynb\n\nFirst I basically refer to your approach for both batchnorm forward and backward propagation, the particular functions are:\nbatchnorm_forward_computational_graph\nbatchnorm_backward_computational_graph\n\nwhile the forward one seems to be working really well, the backward one doesnt work thru the gradient check. just in case if you wanted to try to run my stupid code, you can set the following parameter to \"True\": (it is located at the end of the code)\nglobal_var['checkGradient'] = True\n\nNote that the gradient check need a few iterations to work ok. After some iterations you will see the difference between computational by backward propagation and that of \"manually\" calculating the slope become very small, here is a screenshot:\n https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png \n\nThen I referred to some other examples online and found one that works for the gradient check. I implemented it and now it passes the gradient check. The function is this time around is called:\nbatchnorm_backward_thankGod\n(PS i thankGod for finally be able to pass the checking). As you can see the coding there is way more abstract. I spent quite some time understand the maths behind and the codes. The original source was from: https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n\nI am sure there could be something wrong here but just if you have time, do you mind helping me take a quick look? sorry for the messy codes.\n\nSam","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":4,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T07:14:42","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Sam,\u003cbr>I'm impressed by your efforts to get it correct and your notebook seems like a good playground. But I don't get your question I guess. Why do you assume that your gradient check is failing? The numbers in the image you posted suggest that everything is perfectly fine. You have differences smaller than 1e-7 which is totally fine. You will never get a difference of zero if that is what you are looking for.\u003c/p>\u003cp>More over it makes me curios to know what your gradient check for the other approach results in, if you say this one passes.\u003c/p>\u003cp>Best regards,\u003cbr>Frederik\u003c/p>","id":"3580213422","createdAt":"2017-10-23T07:14:42","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3579327693,"isApproved":true,"isFlagged":false,"raw_message":"Hey Sam,\nI'm impressed by your efforts to get it correct and your notebook seems like a good playground. But I don't get your question I guess. Why do you assume that your gradient check is failing? The numbers in the image you posted suggest that everything is perfectly fine. You have differences smaller than 1e-7 which is totally fine. You will never get a difference of zero if that is what you are looking for. \n\nMore over it makes me curios to know what your gradient check for the other approach results in, if you say this one passes.\n\nBest regards,\nFrederik","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":5,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T09:38:58","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hello Frederik,\u003c/p>\u003cp>my bad! The screenshot i posted in my previous post was actually the result of the \"other\" approach. If I used the computational graph approach, the gradient check will looks like this \u003ca href=\"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png\">https://uploads.disquscdn.c...\u003c/a> :\u003cbr>which shows a relatively larger difference. \u003cbr>Both ways of implementation makes sense to me! \u003cbr>Please if you have a some spare time take a look and please advise if I have done something wrong here.\u003c/p>\u003cp>Many thanks!\u003cbr>Sam\u003c/p>","id":"3580302196","createdAt":"2017-10-23T09:38:58","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","thumbnailUrl":"//uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","htmlHeight":null,"id":32553250,"thumbnailWidth":792,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png"},"urlRedirect":"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","description":"","post":"3580302196","thumbnailURL":"//uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png","thumbnailHeight":330}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3580213422,"isApproved":true,"isFlagged":false,"raw_message":"Hello Frederik,\n\nmy bad! The screenshot i posted in my previous post was actually the result of the \"other\" approach. If I used the computational graph approach, the gradient check will looks like this https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png :\nwhich shows a relatively larger difference. \nBoth ways of implementation makes sense to me! \nPlease if you have a some spare time take a look and please advise if I have done something wrong here.\n\nMany thanks!\nSam","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":6,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T10:41:09","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hmm yes, this is a rather large error and def. shouldn't be this big. But I'm sorry, I won't find time any time soon to take a deeper look over your code or debug it. Maybe post it on reddit ML oder SO and ask their for help.\u003c/p>","id":"3580346830","createdAt":"2017-10-23T10:41:09","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3580302196,"isApproved":true,"isFlagged":false,"raw_message":"Hmm yes, this is a rather large error and def. shouldn't be this big. But I'm sorry, I won't find time any time soon to take a deeper look over your code or debug it. Maybe post it on reddit ML oder SO and ask their for help.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":7,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T10:56:06","dislikes":0,"numReports":0,"likes":18,"message":"\u003cp>yes of course! thank you Frederik!\u003c/p>","id":"3580367804","createdAt":"2017-10-23T10:56:06","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3580346830,"isApproved":true,"isFlagged":false,"raw_message":"yes of course! thank you Frederik!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":8,"points":18,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-27T08:21:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Great blog post. Thank you!!!\u003c/p>","id":"3525944498","createdAt":"2017-09-20T08:21:13","author":{"username":"roeibahumi","about":"","name":"ROEI Bahumi","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2017-09-20T08:20:30","profileUrl":"https://disqus.com/by/roeibahumi/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"265730883","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar32.jpg?1535095095"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar92.jpg?1535095095","large":{"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar92.jpg?1535095095"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Great blog post. Thank you!!!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-27T08:28:31","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks!\u003c/p>","id":"3525949089","createdAt":"2017-09-20T08:28:31","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1537216441/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3525944498,"isApproved":true,"isFlagged":false,"raw_message":"Thanks!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false}],"thread":{"feed":"https://kratzertblog.disqus.com/understanding_the_backward_pass_through_batch_normalization_layer/latest.rss","author":"196251915","dislikes":0,"likes":49,"message":"","isSpam":false,"createdAt":"2016-02-28T01:28:04","category":"4451835","clean_title":"Understanding the backward pass through Batch Normalization Layer","id":"4617794568","signedLink":"https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&key=BEZsXC82hY3NSduqNJmwWQ","isDeleted":false,"hasStreaming":false,"raw_message":"","isClosed":false,"link":"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html","slug":"understanding_the_backward_pass_through_batch_normalization_layer","forum":"kratzertblog","identifiers":[],"posts":114,"moderators":[196251915],"validateAllPosts":false,"title":"Understanding the backward pass through Batch Normalization Layer","highlightedPost":null}},"order":"popular"}</script>


    <div id="fixed-content"></div>


    
        
<script type="text/json" id="disqus-urls">{
    "root":"//disqus.com",
    "next":"https://c.disquscdn.com/next/current/embed"
}</script>

        
        <script>!function () {
            var d = document;
            var toH = d.head.appendChild.bind(d.head);

            var v = window.location.href.match(/[#&?]version=([0-9a-f]{32})/);
            var src = 'https://c.disquscdn.com/next/embed/lounge.load';
            if (v)
                src += '.' + v[1];
            src += '.js';

            var s = d.createElement('script');
            s.crossOrigin = 'anonymous';
            s.id = 'bootstrap-script';
            s.setAttribute('data-app', 'lounge');
            s.src = src;
            toH(s);

            var m = d.createElement('meta');
            m.setAttribute('http-equiv', 'Content-Security-Policy');
            m.setAttribute('content', "script-src https:;");
            toH(m);
        }();</script>
    


<script src="./common.bundle.c11fe52243dba94195dd363cbd3310b9.js.下载"></script><div id="layout" data-tracking-area="layout"><div id="onboard" data-tracking-area="onboard"></div><div id="reactions__container"></div><div id="highlighted-post" data-tracking-area="highlighted" class="highlighted-post" style="display: none;"></div><div id="global-alert"></div><div id="tos__container"></div><header id="main-nav" data-tracking-area="main-nav"><nav class="nav nav-primary"><ul><li class="nav-tab nav-tab--primary tab-conversation active" data-role="post-count"><a class="publisher-nav-color"><span class="comment-count">114 comments</span><span class="comment-count-placeholder">Comments</span></a></li><li class="nav-tab nav-tab--primary tab-community"><a href="https://disqus.com/home/forums/kratzertblog/" class="publisher-nav-color" data-action="community-sidebar" data-forum="kratzertblog" id="community-tab" name="kratzertblog"><span class="community-name"><strong>kratzertblog</strong></span><strong class="community-name-placeholder">Community</strong></a></li><li class="nav-tab nav-tab--primary dropdown user-menu" data-role="logout"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown" role="menuitem" name="Login"><span class="dropdown-toggle-wrapper"><span>Login</span> </span> <span class="caret"></span></a><ul class="dropdown-menu"><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:disqus">Disqus</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:facebook">Facebook</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:twitter">Twitter</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:google">Google</a></li></ul></li><li class="nav-tab nav-tab--primary notification-menu unread" data-role="notification-menu"><a href="https://disqus.com/home/inbox/" class="notification-container" data-action="home" data-home-path="home/inbox/"><span class="notification-icon icon-comment" aria-hidden="true"></span><span class="notification-count" data-role="notification-count">1</span></a></li></ul></nav></header><section id="conversation" data-role="main" data-tracking-area="main"><div class="nav nav-secondary" data-tracking-area="secondary-nav"><ul><li id="recommend-button" class="nav-tab nav-tab--secondary recommend dropdown"><div class="thread-likes"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="recommend" title="Recommend this discussion" class="dropdown-toggle "><span class="label label-default"><span class="recommend-icon icon-heart-empty"></span> Recommend</span><span class="label label-recommended"><span class="recommend-icon icon-heart"></span> Recommended</span> <span class="label label-count">49</span></a><ul class="dropdown-menu dropdown-menu--coachmark"><li><div><h2 class="coachmark__heading">Discussion Recommended!</h2><p class="coachmark__description">Recommending means this is a discussion worth sharing. It gets shared to your followers' Disqus feeds, and gives the creator kudos!</p></div> <a href="https://disqus.com/home/?utm_source=disqus_embed&amp;utm_content=recommend_btn" class="btn btn-primary coachmark__button" target="_blank">Find More Discussions</a></li></ul></div></div></li><li id="thread-share-bar" class="nav-tab nav-tab--secondary share-bar hidden-sm"><div class="thread-share-bar-buttons"><span class="thread-share__button share-twitter" data-action="share:twitter"><span class="icon-twitter"></span><span class="share-text">Tweet</span></span><span class="thread-share__button share-facebook" data-action="share:facebook"><span class="icon-facebook"></span><span class="share-text">Share</span></span></div></li><li data-role="post-sort" class="nav-tab nav-tab--secondary dropdown sorting pull-right"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown">Sort by Best<span class="caret"></span></a><ul class="dropdown-menu pull-right"><li class="selected"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="popular">Best<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="desc">Newest<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="asc">Oldest<i aria-hidden="true" class="icon-checkmark"></i></a></li></ul></li></ul></div><div id="posts"><div id="form" class="textarea-wrapper--top-level"><form class="reply"><div class="postbox"><div role="alert"></div><div class="avatar"><span class="user"><img data-role="user-avatar" src="./noavatar92.7b2fde640943965cc88df0cdee365907.png" alt="Avatar"></span></div><div class="textarea-wrapper" data-role="textarea" dir="auto"><div><span class="placeholder">Join the discussion…</span><div class="textarea" tabindex="0" role="textbox" aria-multiline="true" contenteditable="PLAINTEXT-ONLY" data-role="editable" aria-label="Join the discussion…" style="overflow: auto; overflow-wrap: break-word; max-height: 350px;"><p><br></p></div><div style="display: none;"><ul class="user-mention__list"><li class="header user-mention__header"><h5>in this conversation</h5></li></ul></div></div><div data-role="drag-drop-placeholder" class="media-drag-hover" style="display: none;"><div class="drag-text">⬇ Drag and drop your images here to upload them.</div></div><div class="media-preview empty" data-role="media-preview"><ul data-role="media-progress-list"></ul>
<ul data-role="media-rich-list"></ul>
<div class="media-expanded empty" data-role="media-preview-expanded">
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-role="media-preview-expanded-image" alt="Media preview placeholder">
</div>
</div><div class="edit-alert" role="postbox-alert"></div><div class="post-actions"><ul class="wysiwyg"><li class="wysiwyg__item" data-role="media-uploader"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" tabindex="-1" data-action="attach" class="attach" title="Upload Images"><span>Attach</span></a>
<input type="file" data-role="media-upload" tabindex="-1" accept="image/*">
</li></ul></div></div><div data-role="login-form"><div><div><section class="auth-section logged-out__display"><div class="connect"><h6>Log in with</h6><ul data-role="login-menu" class="services login-buttons"><li class="auth-disqus"><button type="button" data-action="auth:disqus" title="Disqus" class="connect__button"><i class="icon-disqus"></i></button></li><li class="auth-facebook"><button type="button" data-action="auth:facebook" title="Facebook" class="connect__button"><i class="icon-facebook-circle"></i></button></li><li class="auth-twitter"><button type="button" data-action="auth:twitter" title="Twitter" class="connect__button"><i class="icon-twitter-circle"></i></button></li><li class="auth-google"><button type="button" data-action="auth:google" title="Google" class="connect__button"><i class="icon-google-plus-circle"></i></button></li></ul></div><div class="guest"><h6 class="guest-form-title"><span class="register-text"> or sign up with Disqus </span><span class="guest-text"> or pick a name </span></h6> <div class="help-tooltip__wrapper help-icon" tabindex="0"><div id="rules" class="tooltip show help-tooltip"><h3 class="help-tooltip__heading">Disqus is a discussion network</h3><ul class="help-tooltip__list"><li><span>Disqus never moderates or censors. The rules on this community are its own.</span></li><li><span>Don't be a jerk or do anything illegal. Everything is easier that way.</span></li></ul><p class="clearfix"><a href="https://docs.disqus.com/kb/terms-and-policies/" class="btn btn-small help-tooltip__button" target="_blank">Read full terms and conditions</a></p></div></div><p class="input-wrapper"><input dir="auto" type="text" placeholder="Name" name="display_name" id="view103_display_name" maxlength="30" class="input--text" aria-label="name"></p><div class="guest-details " data-role="guest-details"><p class="input-wrapper"><input dir="auto" type="email" placeholder="Email" name="email" id="view103_email" class="input--text" aria-label="email"></p><p class="input-wrapper"><input dir="auto" disabled="" type="text" class="register-text input--text" placeholder="Password" name="password" aria-label="password" id="view103_password"></p><div class="acceptance-wrapper"><p><label><input type="checkbox" name="tos"><span class="spacing-left-small">I agree to Disqus' <a href="https://help.disqus.com/customer/portal/articles/466260-terms-of-service" target="_blank" rel="noopener noreferrer">Terms of Service</a></span></label></p><p><label><input type="checkbox" name="privacy-policy"><span class="spacing-left-small">I agree to Disqus' processing of email and IP address, and the use of cookies, to facilitate my authentication and posting of comments, explained further in the <a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" target="_blank" rel="noopener noreferrer">Privacy Policy</a></span></label></p></div><input type="checkbox" name="author-guest" style="display: none;"><div class="g-recaptcha" data-sitekey="6Lfx6u0SAAAAAI1QkeTW397iQv1MsBfbDaYlwxK_" data-callback="onCaptchaChange" data-expired-callback="onCaptchaChange"></div><div class="proceed" data-role="submit-btn-container"><button type="submit" class="proceed__button btn submit" aria-label="Next" disabled="disabled"><span class="icon-proceed"></span><div class="spinner"></div></button></div></div></div></section></div></div></div></div></form></div><button class="alert alert--realtime" data-role="realtime-notification" style="display: none;"></button><div id="email-signup"></div><div id="no-posts" style="display: none;"></div><ul id="post-list" class="post-list"><li class="post" id="post-3677924058"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_lDJKDl7VxM/" data-action="profile" data-username="disqus_lDJKDl7VxM" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275048753" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_lDJKDl7VxM/" data-action="profile" data-username="disqus_lDJKDl7VxM" target="_blank" rel="noopener noreferrer">JohnMclain</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3677924058" data-role="relative-time" class="time-ago" title="Monday, December 25, 2017 11:28 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>hi，<br>I believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.</p><p>"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions)."</p><p>I think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3677924058"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3708575158"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3677924058" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> JohnMclain</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3708575158" data-role="relative-time" class="time-ago" title="Monday, January 15, 2018 4:40 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3708575158"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3879045238"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/harveyqiu/" data-action="profile" data-username="harveyqiu" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="286645466" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/harveyqiu/" data-action="profile" data-username="harveyqiu" target="_blank" rel="noopener noreferrer">Harvey Qiu</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3708575158" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3879045238" data-role="relative-time" class="time-ago" title="Monday, April 30, 2018 5:04 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew).</p><p>I think the reason for adding up rows is the multivariate-calculus rule which says "gradients flow into different branches of the graph should be summed". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable.</p><p>As a concrete example, in step 9, a term of dL/d\beta should be:<br> <span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder" style="height: 49px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div> <br>where z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.</p><p>However, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3879045238"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3889201765"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3879045238" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Harvey Qiu</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3889201765" data-role="relative-time" class="time-ago" title="Monday, May 7, 2018 2:42 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Harvey,<br>sorry for responding so late. I think the reason you give is absolutely correct but I'm not sure if I get your last question. You mean why I started to disassemble everything in small steps? I saw it in one of the lectures Andrej Kaparthy gave during the CS231n class I linked at the top.  He did the same to explain the backward pass an gradient calculation and I think it is a nice way to make things easier ;)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3889201765"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul></li></ul></li><li class="post" id="post-3638799277"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_cxY7uKUvpl/" data-action="profile" data-username="disqus_cxY7uKUvpl" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="272639134" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_cxY7uKUvpl/" data-action="profile" data-username="disqus_cxY7uKUvpl" target="_blank" rel="noopener noreferrer">aboul</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3638799277" data-role="relative-time" class="time-ago" title="Thursday, November 30, 2017 5:19 AM">10 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks ! the flowchart help me so much to implement in R ;-)<br><span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder" style="height: 513px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3638799277"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3639479740"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3638799277" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> aboul</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3639479740" data-role="relative-time" class="time-ago" title="Thursday, November 30, 2017 4:58 PM">10 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Looks like my sketches, when I first sat down to derive the graph for my self. Good work!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3639479740"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3678638435"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/patricklangechuanliu/" data-action="profile" data-username="patricklangechuanliu" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275100268" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/patricklangechuanliu/" data-action="profile" data-username="patricklangechuanliu" target="_blank" rel="noopener noreferrer">Patrick Langechuan LIU</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3639479740" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3678638435" data-role="relative-time" class="time-ago" title="Tuesday, December 26, 2017 3:18 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi Frederik,</p><p>Thanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout.</p><p>From what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!</p><p>Patrick</p><p>==========<br>Edit: Never mind. I found the explanation on the link you gave at the end of the post <a href="http://disq.us/url?url=http%3A%2F%2Fcthorey.github.io%2Fbackpropagation%2F%3Ar33DVWl8q8rDRF9zRhyrL5527fg&amp;cuid=4023296" rel="nofollow noopener" title="http://cthorey.github.io/backpropagation/">http://cthorey.github.io/ba...</a> in the section "We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...". This is related to the notion of "total derivative" <a href="https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTotal_derivative%3AdO1EtjQujaspBuHilHbsqfsJopg&amp;cuid=4023296" rel="nofollow noopener" title="https://en.wikipedia.org/wiki/Total_derivative">https://en.wikipedia.org/wi...</a>.</p><p>In addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3678638435"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul></li><li class="post" id="post-3772850279"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_GJH1JRIogE/" data-action="profile" data-username="disqus_GJH1JRIogE" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="280979018" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_GJH1JRIogE/" data-action="profile" data-username="disqus_GJH1JRIogE" target="_blank" rel="noopener noreferrer">Jorge</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772850279" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 2:41 AM">7 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks fkratzert for your effort !!<br>I verified your code passes Andrew NG's Gradient Checking.<br>Cheers!!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3772850279"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3772882856"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772850279" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Jorge</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772882856" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 3:00 AM">7 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks man, good to hear ;)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3772882856"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3579296064"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/anandsaha/" data-action="profile" data-username="anandsaha" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="7128646" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/anandsaha/" data-action="profile" data-username="anandsaha" target="_blank" rel="noopener noreferrer">Anand Saha</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579296064" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 12:34 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you! You have a knack of simplifying complexity :-)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3579296064"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3580214271"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579296064" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Anand Saha</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580214271" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 3:16 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580214271"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3561820127"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/majicji/" data-action="profile" data-username="majicji" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="267800662" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/majicji/" data-action="profile" data-username="majicji" target="_blank" rel="noopener noreferrer">Majic Ji</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3561820127" data-role="relative-time" class="time-ago" title="Wednesday, October 11, 2017 3:15 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3561820127"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3578840984"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3561820127" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Majic Ji</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3578840984" data-role="relative-time" class="time-ago" title="Sunday, October 22, 2017 4:31 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Glad I could help you.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3578840984"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3268047638"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="249599616" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer">jschaeff</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268047638" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 10:49 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.<br>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268047638"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3268555117"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268047638" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> jschaeff</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268555117" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 10:23 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? <br>And regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum.</p><p>And for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268555117"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3268622523"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="249599616" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer">jschaeff</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268555117" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268622523" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 11:22 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!</p><p>For the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?</p><p>Thanks again</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268622523"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3268629001"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268622523" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> jschaeff</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268629001" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 11:28 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>So normally the running mean + std from the training data are parameters that should be included in the pretrained weights. These are network parameters, same as gamma and beta. So you don't have to compute them or pass them on each forward pass during inference to the network. As to my knowledge this is done automatically for all of the DL libraries. For TensorFlow e.g. you just have to pass a flag to the BatchNorm-Layer if you are currently training or testing. If you are testing, then the stored moving average will be taken, if not, the moving average will be updated. But again, these parameters are network parameters and should be provided for a pretrained network.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268629001"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3268702865"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="249599616" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer">jschaeff</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268629001" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268702865" data-role="relative-time" class="time-ago" title="Sunday, April 23, 2017 12:26 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Got it.  Thanks again for your clear explanations.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268702865"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3268704115"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268702865" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> jschaeff</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268704115" data-role="relative-time" class="time-ago" title="Sunday, April 23, 2017 12:27 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>You're welcome! Always happy if I can help!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268704115"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li class="post" id="post-3895466348"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_dVXj63cxog/" data-action="profile" data-username="disqus_dVXj63cxog" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="238245013" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_dVXj63cxog/" data-action="profile" data-username="disqus_dVXj63cxog" target="_blank" rel="noopener noreferrer">Michael Green</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895466348" data-role="relative-time" class="time-ago" title="Friday, May 11, 2018 2:09 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Nice writeup. :)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3895466348"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3895523131"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895466348" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Michael Green</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895523131" data-role="relative-time" class="time-ago" title="Friday, May 11, 2018 3:52 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks :)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3895523131"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3874235827"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/abhisheknadgeri/" data-action="profile" data-username="abhisheknadgeri" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="286384918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/abhisheknadgeri/" data-action="profile" data-username="abhisheknadgeri" target="_blank" rel="noopener noreferrer">Abhishek Nadgeri</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3874235827" data-role="relative-time" class="time-ago" title="Friday, April 27, 2018 2:53 AM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the post I loved it :D</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3874235827"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3889199453"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3874235827" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Abhishek Nadgeri</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3889199453" data-role="relative-time" class="time-ago" title="Monday, May 7, 2018 2:37 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>you are welcome. great that you liked it!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3889199453"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3842539330"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/ryanneph/" data-action="profile" data-username="ryanneph" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="284757841" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/ryanneph/" data-action="profile" data-username="ryanneph" target="_blank" rel="noopener noreferrer">Ryan Neph</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3842539330" data-role="relative-time" class="time-ago" title="Saturday, April 7, 2018 4:42 AM">6 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term "derivative", whereas "derivation" instead refers to the process of arriving at a result.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3842539330"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3889199235"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3842539330" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Ryan Neph</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3889199235" data-role="relative-time" class="time-ago" title="Monday, May 7, 2018 2:36 PM">5 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Ryan,<br>sorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3889199235"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3812959741"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="276840001" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer">Jae Duk Seo</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3812959741" data-role="relative-time" class="time-ago" title="Monday, March 19, 2018 1:01 PM">7 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Wow this is such amazing post! Thank you so much!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3812959741"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3773765640"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/vibhujawa/" data-action="profile" data-username="vibhujawa" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="113115540" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/vibhujawa/" data-action="profile" data-username="vibhujawa" target="_blank" rel="noopener noreferrer">Vibhu Jawa</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3773765640" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 4:09 PM">7 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.<br>Loved the clarity of explanation.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3773765640"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3775664434"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3773765640" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Vibhu Jawa</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3775664434" data-role="relative-time" class="time-ago" title="Monday, February 26, 2018 2:05 AM">7 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3775664434"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3749093115"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="276840001" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer">Jae Duk Seo</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3749093115" data-role="relative-time" class="time-ago" title="Friday, February 9, 2018 3:05 AM">8 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Amazing post, thank you for posting!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3749093115"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3738782013"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/hyeungshikjung/" data-action="profile" data-username="hyeungshikjung" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="55165620" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/hyeungshikjung/" data-action="profile" data-username="hyeungshikjung" target="_blank" rel="noopener noreferrer">Hyeungshik Jung</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3738782013" data-role="relative-time" class="time-ago" title="Friday, February 2, 2018 10:05 PM">8 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>What a lovely post ㅜㅜ</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3738782013"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3683913718"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275457335" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer">tapsi</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3683913718" data-role="relative-time" class="time-ago" title="Saturday, December 30, 2017 10:15 AM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I have a problem with "give me back my raw inputs" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3683913718"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3691076340"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3683913718" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> tapsi</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3691076340" data-role="relative-time" class="time-ago" title="Thursday, January 4, 2018 5:51 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3691076340"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3693075455"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275457335" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer">tapsi</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3691076340" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3693075455" data-role="relative-time" class="time-ago" title="Friday, January 5, 2018 11:10 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3693075455"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3697166058"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3693075455" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> tapsi</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3697166058" data-role="relative-time" class="time-ago" title="Monday, January 8, 2018 4:35 PM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Would be nice to hear of your results, once you can show off something (also negative results please ;) )</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3697166058"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul></li></ul></li><li class="post" id="post-3675016359"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/kanhavishva/" data-action="profile" data-username="kanhavishva" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="159548160" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/kanhavishva/" data-action="profile" data-username="kanhavishva" target="_blank" rel="noopener noreferrer">kanhavishva</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3675016359" data-role="relative-time" class="time-ago" title="Saturday, December 23, 2017 6:20 AM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Awesome work, helped a lot, thank you buddy..</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3675016359"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3676081514"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3675016359" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> kanhavishva</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3676081514" data-role="relative-time" class="time-ago" title="Sunday, December 24, 2017 3:54 AM">9 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3676081514"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li><li class="post" id="post-3631354318"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/rakhilimmidisetti/" data-action="profile" data-username="rakhilimmidisetti" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="272151587" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/rakhilimmidisetti/" data-action="profile" data-username="rakhilimmidisetti" target="_blank" rel="noopener noreferrer">Rakhil Immidisetti</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3631354318" data-role="relative-time" class="time-ago" title="Saturday, November 25, 2017 2:11 AM">10 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3631354318"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3627668880"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/zhuzii/" data-action="profile" data-username="zhuzii" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="197538510" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/zhuzii/" data-action="profile" data-username="zhuzii" target="_blank" rel="noopener noreferrer">zhuzii</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3627668880" data-role="relative-time" class="time-ago" title="Wednesday, November 22, 2017 2:14 PM">10 months ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Cool, thanks.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3627668880"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3570397972"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_nZ5GrhxFcX/" data-action="profile" data-username="disqus_nZ5GrhxFcX" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="112372310" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_nZ5GrhxFcX/" data-action="profile" data-username="disqus_nZ5GrhxFcX" target="_blank" rel="noopener noreferrer">Writer</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3570397972" data-role="relative-time" class="time-ago" title="Tuesday, October 17, 2017 5:19 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.</p><p>The description is at <a href="http://disq.us/url?url=http%3A%2F%2FmiabellaAI.net%2F%3AmZg9hp0mP9jfL2lQ_spC2Vavfxs&amp;cuid=4023296" rel="nofollow noopener" title="http://miabellaAI.net/">http://miabellaAI.net/</a> and the web app is here: <a href="http://disq.us/url?url=http%3A%2F%2Fann.miabellaAI.net%2F%3AXkKD8PC7tnMuxD7JHLis1E4SN3Q&amp;cuid=4023296" rel="nofollow noopener" title="http://ann.miabellaAI.net/">http://ann.miabellaAI.net/</a></p><p>Here are some tips for those who took Andrew Ng’s deep learning course:</p><p>1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.</p><p>2. The Python code I’ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.</p><p>3. Gradient checking can be a pain to write, but it’s the main way to make sure everything is hooked up correctly.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3570397972"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li><li class="post" id="post-3531385013"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531385013" data-role="relative-time" class="time-ago" title="Saturday, September 23, 2017 7:23 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: <a href="https://disq.us/url?url=https%3A%2F%2Fkevinzakka.github.io%2F2016%2F09%2F14%2Fbatch_normalization%2F%3AtMo4ZSbuJOQUKdX33UaO9WftIFM&amp;cuid=4023296" rel="nofollow noopener" title="https://kevinzakka.github.io/2016/09/14/batch_normalization/">https://kevinzakka.github.i...</a> and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531385013"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3531630005"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531385013" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Samuel Pun</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531630005" data-role="relative-time" class="time-ago" title="Saturday, September 23, 2017 11:28 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Samuel,<br>there are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. <br>E.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating <br>f'(x) = (f(x + delta_x) - f(x)) / delta_x<br>and then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531630005"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3531808372"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531630005" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531808372" data-role="relative-time" class="time-ago" title="Sunday, September 24, 2017 1:30 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531808372"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3578839022"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531808372" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Samuel Pun</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3578839022" data-role="relative-time" class="time-ago" title="Sunday, October 22, 2017 4:27 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>let me know if it worked for you!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3578839022"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3579327693"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3578839022" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579327693" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 12:58 AM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container" style="height: 374px;"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Dear Fkratzert,</p><p>Thank you very much for following up with me. Again I am a newbie I guess there is a high chance that I was wrong. I spent quite some time on this issue last month but I still couldnt figure out what went wrong.</p><p>I tried 2 approaches and I uploaded my code here:<br><a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Fchihoxtra%2FMachineLearning%2Fblob%2Fmaster%2F%28Local%29%2520Neuro%2520Network%2520PlayAround.ipynb%3AV7JY7q1_eimdpzwk0dovYf97CtQ&amp;cuid=4023296" rel="nofollow noopener" title="https://github.com/chihoxtra/MachineLearning/blob/master/(Local)%20Neuro%20Network%20PlayAround.ipynb">https://github.com/chihoxtr...</a></p><p>First I basically refer to your approach for both batchnorm forward and backward propagation, the particular functions are:<br>batchnorm_forward_computational_graph<br>batchnorm_backward_computational_graph</p><p>while the forward one seems to be working really well, the backward one doesnt work thru the gradient check. just in case if you wanted to try to run my stupid code, you can set the following parameter to "True": (it is located at the end of the code)<br>global_var['checkGradient'] = True</p><p>Note that the gradient check need a few iterations to work ok. After some iterations you will see the difference between computational by backward propagation and that of "manually" calculating the slope become very small, here is a screenshot:<br> <span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/92afd737cc094e392e0966bf6bdd76da11e7777efe2d4855207b4b8c10e814ef.png" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder" style="height: 229px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div></p><p>Then I referred to some other examples online and found one that works for the gradient check. I implemented it and now it passes the gradient check. The function is this time around is called:<br>batchnorm_backward_thankGod<br>(PS i thankGod for finally be able to pass the checking). As you can see the coding there is way more abstract. I spent quite some time understand the maths behind and the codes. The original source was from: <a href="https://disq.us/url?url=https%3A%2F%2Fwiseodd.github.io%2Ftechblog%2F2016%2F07%2F04%2Fbatchnorm%2F%3ARK0L7aVOAtxP6h4cXjYYKZc3tuU&amp;cuid=4023296" rel="nofollow noopener" title="https://wiseodd.github.io/techblog/2016/07/04/batchnorm/">https://wiseodd.github.io/t...</a></p><p>I am sure there could be something wrong here but just if you have time, do you mind helping me take a quick look? sorry for the messy codes.</p><p>Sam</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3579327693"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3580213422"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579327693" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Samuel Pun</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580213422" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 3:14 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Sam,<br>I'm impressed by your efforts to get it correct and your notebook seems like a good playground. But I don't get your question I guess. Why do you assume that your gradient check is failing? The numbers in the image you posted suggest that everything is perfectly fine. You have differences smaller than 1e-7 which is totally fine. You will never get a difference of zero if that is what you are looking for.</p><p>More over it makes me curios to know what your gradient check for the other approach results in, if you say this one passes.</p><p>Best regards,<br>Frederik</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580213422"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3580302196"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580213422" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580302196" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 5:38 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hello Frederik,</p><p>my bad! The screenshot i posted in my previous post was actually the result of the "other" approach. If I used the computational graph approach, the gradient check will looks like this <span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/2a4a3ca26e548302d522468cf4ec64c15f05a5ecd2155ccdbc23dccdb90a4156.png" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder" style="height: 227px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div> :<br>which shows a relatively larger difference. <br>Both ways of implementation makes sense to me! <br>Please if you have a some spare time take a look and please advise if I have done something wrong here.</p><p>Many thanks!<br>Sam</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580302196"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3580346830"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580302196" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Samuel Pun</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580346830" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 6:41 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hmm yes, this is a rather large error and def. shouldn't be this big. But I'm sorry, I won't find time any time soon to take a deeper look over your code or debug it. Maybe post it on reddit ML oder SO and ask their for help.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580346830"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3580367804"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580346830" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580367804" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 6:56 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>yes of course! thank you Frederik!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-18" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">18</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580367804"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li class="post" id="post-3525944498"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/roeibahumi/" data-action="profile" data-username="roeibahumi" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="265730883" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/roeibahumi/" data-action="profile" data-username="roeibahumi" target="_blank" rel="noopener noreferrer">ROEI Bahumi</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525944498" data-role="relative-time" class="time-ago" title="Wednesday, September 20, 2017 4:21 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Great blog post. Thank you!!!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3525944498"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"><li class="post" id="post-3525949089"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525944498" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> ROEI Bahumi</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525949089" data-role="relative-time" class="time-ago" title="Wednesday, September 20, 2017 4:28 PM">a year ago</a></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3525949089"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><ul data-role="children" class="children"></ul></li></ul></li></ul><div class="load-more" data-role="more" style=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="more-posts" class="btn load-more__button">Load more comments</a></div></div></section><div id="placement-bottom" data-tracking-area="discovery-south"><div class="post-list" style="height: auto; visibility: visible;"><div style="display: block; width: 100%;"><div id="discovery-main-c234" class="discovery-main"><section id="col-organic-c1147" class="col-organic"><header class="discovery-col-header"><h2>Also on <strong>kratzertblog</strong></h2></header><ul class="discovery-posts" data-role="discovery-posts"><li class="discovery-post post-0" id="discovery-link-organic-5914010462"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F06%2F15%2Fexample-of-tensorflows-new-input-pipeline.html&amp;key=eLW9n1InH-7UKCOGmUbvxw" target="" rel=""><header class="discovery-post-header"><h3 title="Example of TensorFlows new Input Pipeline"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Example of TensorFlows new Input Pipeline</span></h3><ul class="meta"><li class="comments">41 comments </li> <li class="time">a year ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F06%2F15%2Fexample-of-tensorflows-new-input-pipeline.html&amp;key=eLW9n1InH-7UKCOGmUbvxw" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">Akshay Aradhya</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3">Your second link is broken.Correct Link : https://towardsdatascience....</span></p></a></a></li><li class="discovery-post post-1" id="discovery-link-organic-6169758829"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F25%2Fnumba-series-part-2-custom-data-types-and-parallelization.html&amp;key=1YVLy_WsZAmlmUJE710CLg" target="" rel=""><header class="discovery-post-header"><h3 title="Numba series part 2: Custom data types and parallelization"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Numba series part 2: Custom data types and parallelization</span></h3><ul class="meta"><li class="comments">4 comments </li> <li class="time">a year ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F25%2Fnumba-series-part-2-custom-data-types-and-parallelization.html&amp;key=1YVLy_WsZAmlmUJE710CLg" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">fkratzert</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3">You don't have to use Anaconda for using Numba (I guess you know). The library is just developed by Anaconda.</span></p></a></a></li><li class="discovery-post post-2" id="discovery-link-organic-4573555772"><a class="publisher-anchor-color" href="http://disq.us/?url=http%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;key=RSXVtwUwchblzmH6d4JEZw" target="" rel=""><header class="discovery-post-header"><h3 title="Understanding the backwardpass through Batch Normalization Layer"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Understanding the backwardpass through Batch Normalization Layer</span></h3><ul class="meta"><li class="comments">27 comments </li> <li class="time">3 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="http://disq.us/?url=http%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;key=RSXVtwUwchblzmH6d4JEZw" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">Yuval Pinter</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3">A new paper offers new SELU function, claiming batch normalization is no longer necessary.https://arxiv.org/abs/1706....</span></p></a></a></li><li class="discovery-post post-3" id="discovery-link-organic-5581251971"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F02%2F24%2Ffinetuning-alexnet-with-tensorflow.html&amp;key=cFMiDsxyTE9QfLNmwwGMJg" target="" rel=""><header class="discovery-post-header"><h3 title="Finetuning AlexNet with TensorFlow"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Finetuning AlexNet with TensorFlow</span></h3><ul class="meta"><li class="comments">185 comments </li> <li class="time">2 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F02%2F24%2Ffinetuning-alexnet-with-tensorflow.html&amp;key=cFMiDsxyTE9QfLNmwwGMJg" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">Hugo</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3"> Hi,First an foremost, thanks for this neat &amp; tidy tutorial. Coming from Caffe, Tensorflow can be cumbersome. It works great on my own data and …</span></p></a></a></li></ul></section></div></div></div></div><div id="footer" data-tracking-area="footer" class="disqus-footer__wrapper"><ul class="disqus-footer"><li class="disqus-footer__logo"><a href="https://disqus.com/" rel="nofollow" title="Powered by Disqus" class="disqus-footer__link">Powered by Disqus</a></li><li id="thread-subscribe-button" class="email disqus-footer__item"><div class="default"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" rel="nofollow" data-action="subscribe" title="Subscribe and get email updates from this discussion" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-mail"></i><span class="clip">Subscribe</span><i aria-hidden="true" class="icon icon-checkmark"></i></a></div></li><li class="install disqus-footer__item"><a href="https://publishers.disqus.com/engage?utm_source=kratzertblog&amp;utm_medium=Disqus-Footer" rel="nofollow" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-disqus"></i><span class="clip hidden-md">Add Disqus to your site</span><span class="clip visible-md hidden-xs">Add Disqus</span><span class="clip visible-xs">Add</span></a></li><li class="privacy disqus-footer__item"><a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" rel="nofollow" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-lock"></i><span class="clip hidden-sm">Disqus' Privacy Policy</span><span class="clip visible-sm hidden-xs">Privacy Policy</span><span class="clip visible-xs">Privacy</span></a></li></ul></div></div><div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div></div></div><div style="position: absolute; top: -10000px; height: 0px; width: 0px;"><div><iframe name="fb_xdm_frame_https" frameborder="0" allowtransparency="true" allowfullscreen="true" scrolling="no" allow="encrypted-media" id="fb_xdm_frame_https" aria-hidden="true" title="Facebook Cross Domain Communication Frame" tabindex="-1" src="./jeGFvz3E6vx.html" style="border: none;"></iframe></div></div></div><iframe id="ssIFrame_google" sandbox="allow-scripts allow-same-origin" aria-hidden="true" frame-border="0" src="./iframe.html" style="position: absolute; width: 1px; height: 1px; left: -9999px; top: -9999px; right: -9999px; bottom: -9999px; display: none;"></iframe></body></html>